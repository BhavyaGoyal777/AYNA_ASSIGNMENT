{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291d7ad4-f507-4497-952f-d24e934cacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Define transforms\n",
    "resize_transform = transforms.Resize((128, 128))\n",
    "to_tensor_transform = transforms.ToTensor()\n",
    "normalize_transform = transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "\n",
    "def paired_augmentation(input_img, output_img):\n",
    "    if random.random() < 0.5:\n",
    "        input_img = transforms.functional.hflip(input_img)\n",
    "        output_img = transforms.functional.hflip(output_img)\n",
    "    if random.random() < 0.5:\n",
    "        input_img = transforms.functional.vflip(input_img)\n",
    "        output_img = transforms.functional.vflip(output_img)\n",
    "    angle = random.uniform(-15, 15)\n",
    "    input_img = transforms.functional.rotate(input_img, angle)\n",
    "    output_img = transforms.functional.rotate(output_img, angle)\n",
    "    input_img = color_jitter(input_img)\n",
    "    return input_img, output_img\n",
    "\n",
    "# PyTorch Dataset class\n",
    "class HFDataset(Dataset):\n",
    "    def __init__(self, hf_ds, augment=False):\n",
    "        self.ds = hf_ds\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        input_img = item[\"input\"].convert(\"RGB\")\n",
    "        output_img = item[\"output\"].convert(\"RGB\")\n",
    "        prompt = item[\"prompt\"].lower()\n",
    "\n",
    "        input_img = resize_transform(input_img)\n",
    "        output_img = resize_transform(output_img)\n",
    "\n",
    "        if self.augment:\n",
    "            input_img, output_img = paired_augmentation(input_img, output_img)\n",
    "\n",
    "        input_img = to_tensor_transform(input_img)\n",
    "        output_img = to_tensor_transform(output_img)\n",
    "\n",
    "        input_img = normalize_transform(input_img)\n",
    "        output_img = normalize_transform(output_img)\n",
    "\n",
    "        return {\n",
    "            \"input_image\": input_img,\n",
    "            \"output_image\": output_img,\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "\n",
    "# 1. Load synthetic dataset and take first 3000\n",
    "synthetic_raw = load_dataset(\"bhavya777/synthetic-colored-shapes\")\n",
    "synthetic_ds = synthetic_raw[\"train\"].select(range(4000))\n",
    "synthetic_pt = HFDataset(synthetic_ds, augment=True)\n",
    "\n",
    "# 2. Load augmented dataset (no extra augment)\n",
    "augmented_raw = load_dataset(\"bhavya777/augmented-colored-shapes\")\n",
    "augmented_pt = HFDataset(augmented_raw[\"train\"], augment=False)\n",
    "\n",
    "# 3. Combine\n",
    "full_dataset = ConcatDataset([synthetic_pt, augmented_pt])\n",
    "\n",
    "# 4. Split\n",
    "total_len = len(full_dataset)\n",
    "train_len = int(0.8 * total_len)\n",
    "val_len = total_len - train_len\n",
    "\n",
    "train_ds, val_ds = random_split(full_dataset, [train_len, val_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# 5. Dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6fec03-191f-4918-8488-7ceb3c61325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from diffusers import UNet2DConditionModel\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "class ColorDenoisingUNet(nn.Module):\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CLIP text encoder\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "        # UNet architecture\n",
    "        self.unet = UNet2DConditionModel(\n",
    "            sample_size=128,\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            down_block_types=(\n",
    "                \"DownBlock2D\",\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "            ),\n",
    "            mid_block_type=\"UNetMidBlock2DCrossAttn\",\n",
    "            up_block_types=(\n",
    "                \"CrossAttnUpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\",\n",
    "                \"UpBlock2D\",\n",
    "            ),\n",
    "            # block_out_channels=(32,64, 128),\n",
    "            block_out_channels=(64,32,64),\n",
    "            \n",
    "            # block_out_channels=(64, 128,256),\n",
    "            layers_per_block=1,\n",
    "            attention_head_dim=12,\n",
    "            cross_attention_dim=512,\n",
    "            only_cross_attention=True,\n",
    "            dropout=0.1,\n",
    "            act_fn=\"silu\",\n",
    "        ).to(device)\n",
    "\n",
    "    def get_text_embedding(self, prompt, x):\n",
    "        if isinstance(prompt, str):\n",
    "            prompt = [prompt] * x.size(0)\n",
    "\n",
    "        text_inputs = self.tokenizer(\n",
    "            prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        text_embeddings = self.text_encoder(**text_inputs).last_hidden_state\n",
    "        return text_embeddings  # (B, 77, 512)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, timestep: torch.Tensor, prompt):\n",
    "        # print(f\"[DEBUG] Input x shape: {x.shape}\")               # (B, 3, 128, 128)\n",
    "        # print(f\"[DEBUG] Timestep shape: {timestep.shape}\")       # (B,)\n",
    "        # print(f\"[DEBUG] Prompt: {prompt}\")                       # batched prompt or string\n",
    "\n",
    "        encoder_hidden_states = self.get_text_embedding(prompt, x)\n",
    "        # print(f\"[DEBUG] Encoder hidden states shape: {encoder_hidden_states.shape}\")  # (B, 77, 512)\n",
    "\n",
    "        out = self.unet(\n",
    "            sample=x,\n",
    "            timestep=timestep,\n",
    "            encoder_hidden_states=encoder_hidden_states\n",
    "        ).sample\n",
    "\n",
    "        # print(f\"[DEBUG] Output shape: {out.shape}\")              # Should be (B, 3, 128, 128)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae9a37fe-e7f9-49f4-bbef-055eb6f24bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import wandb\n",
    "import piq\n",
    "import lpips\n",
    "from kornia.color import rgb_to_lab\n",
    "\n",
    "# ---------------- CONFIGS ----------------\n",
    "\n",
    "loss_config = {\n",
    "    \"mse\": True,\n",
    "    \"lpips\": True,\n",
    "    \"ssim\": True,\n",
    "    \"color\": True\n",
    "}\n",
    "loss_weights = {\n",
    "    \"mse\": 1.0,\n",
    "    \"lpips\": 0.5,\n",
    "    \"ssim\": 0.2,\n",
    "    \"color\": 0.3\n",
    "}\n",
    "device = \"cuda\"\n",
    "epochs = 5  # Change this as needed\n",
    "\n",
    "# ---------------- LOSSES ----------------\n",
    "\n",
    "def color_loss_lab(output, target):\n",
    "    output = torch.clamp((output + 1) / 2, 0, 1)\n",
    "    target = torch.clamp((target + 1) / 2, 0, 1)\n",
    "    return F.l1_loss(rgb_to_lab(output), rgb_to_lab(target))\n",
    "\n",
    "# ---------------- OPTIMIZER & SCHEDULER ----------------\n",
    "\n",
    "def get_optimizer_and_scheduler(model, train_dataloader, epochs, base_lr=1e-4, wd=1e-4, warmup_ratio=0.1):\n",
    "    optimizer = AdamW(model.parameters(), lr=base_lr, weight_decay=wd)\n",
    "    steps_per_epoch = len(train_dataloader)\n",
    "    total_steps = epochs * steps_per_epoch\n",
    "    warmup_steps = int(warmup_ratio * total_steps)\n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(progress * math.pi))\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "    return optimizer, scheduler, total_steps, warmup_steps\n",
    "\n",
    "# ---------------- TRAIN LOOP ----------------\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader=None,\n",
    "                epochs=5, device=\"cuda\",\n",
    "                loss_config=None, loss_weights=None,\n",
    "                base_lr=1e-4, wd=1e-4, warmup_ratio=0.1,save_dir = \"chuchu\"):\n",
    "\n",
    "    model.to(device)\n",
    "    lpips_loss = lpips.LPIPS(net='vgg').to(device)\n",
    "    optimizer, scheduler, total_steps, warmup_steps = get_optimizer_and_scheduler(\n",
    "        model, train_dataloader, epochs, base_lr, wd, warmup_ratio)\n",
    "    wandb.init(\n",
    "        project=\"assignment-color-adding-unet\",\n",
    "        name=\"head-12(64,32,64)_colorL-ssim-lpips-mse-smol-unet-15-epochs\"\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "\n",
    "        for batch in loop:\n",
    "            input_image = batch[\"input_image\"].to(device)\n",
    "            output_image = batch[\"output_image\"].to(device)\n",
    "            prompt = batch[\"prompt\"]\n",
    "            timestep = torch.zeros(input_image.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "            outputs = model(input_image, timestep=timestep, prompt=prompt)\n",
    "            outputs_01 = torch.clamp((outputs + 1) / 2, 0, 1)\n",
    "            output_image_01 = torch.clamp((output_image + 1) / 2, 0, 1)\n",
    "\n",
    "            # Compute selected losses\n",
    "            loss = 0.0\n",
    "            log_dict = {}\n",
    "            if loss_config.get(\"mse\"):\n",
    "                mse = F.mse_loss(outputs, output_image)\n",
    "                loss += loss_weights[\"mse\"] * mse\n",
    "                log_dict[\"batch_mse\"] = mse.item()\n",
    "            if loss_config.get(\"lpips\"):\n",
    "                lp = lpips_loss(outputs, output_image).mean()\n",
    "                loss += loss_weights[\"lpips\"] * lp\n",
    "                log_dict[\"batch_lpips\"] = lp.item()\n",
    "            if loss_config.get(\"ssim\"):\n",
    "                ssim = piq.ssim(outputs_01, output_image_01, data_range=1.0).mean()\n",
    "                loss += loss_weights[\"ssim\"] * (1 - ssim)\n",
    "                log_dict[\"batch_ssim\"] = ssim.item()\n",
    "            if loss_config.get(\"color\"):\n",
    "                col = color_loss_lab(outputs, output_image)\n",
    "                loss += loss_weights[\"color\"] * col\n",
    "                log_dict[\"batch_color_loss\"] = col.item()\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "            log_dict[\"batch_loss\"] = loss.item()\n",
    "            log_dict[\"epoch\"] = epoch + 1\n",
    "            wandb.log(log_dict)\n",
    "\n",
    "        # End of epoch logging\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        wandb.log({\"train_loss\": avg_train_loss, \"epoch\": epoch + 1})\n",
    "        print(f\"Epoch {epoch+1} avg train loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # ---------- Validation & Image Saving ----------\n",
    "        if val_dataloader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_lpips_total = 0.0\n",
    "            val_mse_total = 0.0\n",
    "            val_ssim_total = 0.0\n",
    "            val_color_total = 0.0\n",
    "            grid = None\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, val_batch in enumerate(tqdm(val_dataloader, desc=f\"Epoch {epoch+1} [Val]\")):\n",
    "                    val_input = val_batch[\"input_image\"].to(device)\n",
    "                    val_target = val_batch[\"output_image\"].to(device)\n",
    "                    val_prompt = val_batch[\"prompt\"]\n",
    "                    val_timestep = torch.zeros(val_input.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "                    val_output = model(val_input, timestep=val_timestep, prompt=val_prompt)\n",
    "                    val_output_01 = torch.clamp((val_output + 1) / 2, 0, 1)\n",
    "                    val_target_01 = torch.clamp((val_target + 1) / 2, 0, 1)\n",
    "\n",
    "                    val_mse = F.mse_loss(val_output, val_target)\n",
    "                    val_lpips = lpips_loss(val_output, val_target).mean()\n",
    "                    val_ssim = piq.ssim(val_output_01, val_target_01, data_range=1.0).mean()\n",
    "                    val_col_loss = color_loss_lab(val_output, val_target)\n",
    "\n",
    "                    val_combined_loss = (\n",
    "                        (loss_weights[\"mse\"] * val_mse if loss_config.get(\"mse\") else 0) +\n",
    "                        (loss_weights[\"lpips\"] * val_lpips if loss_config.get(\"lpips\") else 0) +\n",
    "                        (loss_weights[\"ssim\"] * (1 - val_ssim) if loss_config.get(\"ssim\") else 0) +\n",
    "                        (loss_weights[\"color\"] * val_col_loss if loss_config.get(\"color\") else 0)\n",
    "                    )\n",
    "\n",
    "                    val_loss += val_combined_loss.item()\n",
    "                    val_mse_total += val_mse.item()\n",
    "                    val_lpips_total += val_lpips.item()\n",
    "                    val_ssim_total += val_ssim.item()\n",
    "                    val_color_total += val_col_loss.item()\n",
    "\n",
    "                    # Save images from first batch only\n",
    "                    if batch_idx == 0:\n",
    "                        save_dir = save_dir\n",
    "                        os.makedirs(save_dir, exist_ok=True)\n",
    "                        n = min(4, val_output.size(0))\n",
    "                        outputs = val_output[:n]\n",
    "                        gts = val_target[:n]\n",
    "                        # Interleave output and GT: [output1, gt1, output2, gt2, ...]\n",
    "                        pairs = []\n",
    "                        for i in range(n):\n",
    "                            pairs.append(outputs[i])\n",
    "                            pairs.append(gts[i])\n",
    "                        comparison = torch.stack(pairs, dim=0)\n",
    "                        grid = make_grid(comparison, nrow=2*n, normalize=True, value_range=(-1, 1))\n",
    "                        save_image(grid, f\"{save_dir}/epoch_{epoch+1}.png\")\n",
    "                        wandb.log({f\"val_images_epoch_{epoch+1}\": wandb.Image(grid)})\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "            avg_val_mse = val_mse_total / len(val_dataloader)\n",
    "            avg_val_lpips = val_lpips_total / len(val_dataloader)\n",
    "            avg_val_ssim = val_ssim_total / len(val_dataloader)\n",
    "            avg_val_color = val_color_total / len(val_dataloader)\n",
    "\n",
    "            wandb.log({\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_mse\": avg_val_mse,\n",
    "                \"val_lpips\": avg_val_lpips,\n",
    "                \"val_ssim\": avg_val_ssim,\n",
    "                \"val_color_loss\": avg_val_color,\n",
    "                \"epoch\": epoch + 1\n",
    "            })\n",
    "            print(f\"Epoch {epoch+1} avg val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# ---------------- USAGE EXAMPLE ----------------\n",
    "\n",
    "# Assume you have your model, train_dataloader, val_dataloader defined\n",
    "# model = ColorDenoisingUNet().to(device)\n",
    "# train_dataloader = ...\n",
    "# val_dataloader = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "193d668f-39d2-4446-b581-aa6cca5ad8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_color_loss</td><td>▇▆█▆▆▇▇▆█▅▆▆▆▅▆▅▆▄▃▅▄▄▄▄▂▃▃▂▄▃▂▂▂▂▂▁▁▂</td></tr><tr><td>batch_loss</td><td>█▆█▆▆▇▇▆█▅▆▆▆▅▆▅▆▄▄▅▄▄▄▄▂▃▃▂▄▃▂▂▂▂▂▁▁▁</td></tr><tr><td>batch_lpips</td><td>▇▅▇█▇██▆█▅▆▅▇▆█▆▇▆▄▅▅▅▅▄▄▃▅▄▆▃▄▃▁▂▃▁▁▂</td></tr><tr><td>batch_mse</td><td>█▅█▇▅▆▅▆▇▅▆▆█▆▆▆▆▄▆▆▅▅▆▆▅▆▆▄▄▅▃▄▅▃▃▂▂▁</td></tr><tr><td>batch_ssim</td><td>▂▃▂▄▄▄▃▄▁▆▃▃▄▆▄▃▂▃▅▂▅▄▄▄▇▅▄▆▄▅▆▅▅▅▆█▆▇</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_color_loss</td><td>25.73924</td></tr><tr><td>batch_loss</td><td>9.2669</td></tr><tr><td>batch_lpips</td><td>0.7278</td></tr><tr><td>batch_mse</td><td>1.05133</td></tr><tr><td>batch_ssim</td><td>0.35049</td></tr><tr><td>epoch</td><td>1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">head-8_colorL-ssim-lpips-mse-smol-unet-15-epochs</strong> at: <a href='https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet/runs/uv0bltjn' target=\"_blank\">https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet/runs/uv0bltjn</a><br> View project at: <a href='https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet' target=\"_blank\">https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250805_060859-uv0bltjn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250805_061004-lk4p7vhd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet/runs/lk4p7vhd' target=\"_blank\">head-12(64,32,64)_colorL-ssim-lpips-mse-smol-unet-15-epochs</a></strong> to <a href='https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet' target=\"_blank\">https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet/runs/lk4p7vhd' target=\"_blank\">https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet/runs/lk4p7vhd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|██████████| 540/540 [02:48<00:00,  3.20it/s, loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg train loss: 2.4073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Val]: 100%|██████████| 135/135 [00:16<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg val loss: 0.2497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]: 100%|██████████| 540/540 [02:49<00:00,  3.19it/s, loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg train loss: 0.2779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Val]: 100%|██████████| 135/135 [00:16<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg val loss: 0.1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]: 100%|██████████| 540/540 [02:49<00:00,  3.18it/s, loss=0.177] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg train loss: 0.1545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Val]: 100%|██████████| 135/135 [00:16<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg val loss: 0.1112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]: 100%|██████████| 540/540 [02:48<00:00,  3.20it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg train loss: 0.1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Val]: 100%|██████████| 135/135 [00:16<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg val loss: 0.0926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Val]: 100%|██████████| 135/135 [00:17<00:00,  7.82it/s]it/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 avg val loss: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Train]: 100%|██████████| 540/540 [02:49<00:00,  3.18it/s, loss=0.0591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 avg train loss: 0.0789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Val]: 100%|██████████| 135/135 [00:17<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 avg val loss: 0.0575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Train]: 100%|██████████| 540/540 [02:49<00:00,  3.18it/s, loss=0.0867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 avg train loss: 0.0712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Val]: 100%|██████████| 135/135 [00:17<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 avg val loss: 0.0561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Train]: 100%|██████████| 540/540 [02:48<00:00,  3.20it/s, loss=0.057] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 avg train loss: 0.0628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Val]: 100%|██████████| 135/135 [00:17<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 avg val loss: 0.0465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Train]: 100%|██████████| 540/540 [02:47<00:00,  3.22it/s, loss=0.0552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 avg train loss: 0.0571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Val]: 100%|██████████| 135/135 [00:16<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 avg val loss: 0.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Train]: 100%|██████████| 540/540 [02:48<00:00,  3.20it/s, loss=0.0497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 avg train loss: 0.0530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Val]: 100%|██████████| 135/135 [00:17<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 avg val loss: 0.0397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Train]: 100%|██████████| 540/540 [02:48<00:00,  3.21it/s, loss=0.0357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 avg train loss: 0.0479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Val]: 100%|██████████| 135/135 [00:16<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 avg val loss: 0.0382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [Train]: 100%|██████████| 540/540 [02:47<00:00,  3.23it/s, loss=0.0426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 avg train loss: 0.0452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Val]: 100%|██████████| 135/135 [00:17<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 avg val loss: 0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [Train]: 100%|██████████| 540/540 [02:48<00:00,  3.21it/s, loss=0.0486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 avg train loss: 0.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Val]: 100%|██████████| 135/135 [00:16<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 avg val loss: 0.0336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [Train]: 100%|██████████| 540/540 [02:47<00:00,  3.23it/s, loss=0.0455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 avg train loss: 0.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Val]: 100%|██████████| 135/135 [00:16<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 avg val loss: 0.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [Train]: 100%|██████████| 540/540 [02:47<00:00,  3.23it/s, loss=0.0418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 avg train loss: 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Val]: 100%|██████████| 135/135 [00:16<00:00,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 avg val loss: 0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_color_loss</td><td>▇█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_loss</td><td>█▇▆▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_lpips</td><td>█▆▅▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_mse</td><td>█▆▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_ssim</td><td>▁▃▇▆▇▇▅▇▆▇▇▇██▇▇▇███▇█▇█████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_color_loss</td><td>█▄▄▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_lpips</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mse</td><td>█▄▃▃▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_ssim</td><td>▁▆▆▇▆▇█▇███████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_color_loss</td><td>0.08932</td></tr><tr><td>batch_loss</td><td>0.04177</td></tr><tr><td>batch_lpips</td><td>0.01992</td></tr><tr><td>batch_mse</td><td>0.00409</td></tr><tr><td>batch_ssim</td><td>0.9954</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss</td><td>0.04189</td></tr><tr><td>val_color_loss</td><td>0.07279</td></tr><tr><td>val_loss</td><td>0.03301</td></tr><tr><td>val_lpips</td><td>0.01248</td></tr><tr><td>val_mse</td><td>0.00422</td></tr><tr><td>val_ssim</td><td>0.99642</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">head-12(64,32,64)_colorL-ssim-lpips-mse-smol-unet-15-epochs</strong> at: <a href='https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet/runs/lk4p7vhd' target=\"_blank\">https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet/runs/lk4p7vhd</a><br> View project at: <a href='https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet' target=\"_blank\">https://wandb.ai/777bhavya-dwarkadas-j-sanghvi-college-of-engineering/assignment-color-adding-unet</a><br>Synced 5 W&B file(s), 15 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250805_061004-lk4p7vhd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ColorDenoisingUNet().to(device)\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=15,\n",
    "    device=device,\n",
    "    loss_config=loss_config,\n",
    "    loss_weights=loss_weights,\n",
    "    base_lr=1e-4,\n",
    "    wd=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    save_dir = \"head-12(64,32,64)_colorL-ssim-lpips-mse-smol-unet-15-epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a04ad5-a898-4b91-867b-7546ae67f0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ee9196a3414b92bf5cea57c85f6285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcf0c0f402a4a57b667e81930db9dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d38105ff6242ef800359a408b39a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ol-unet-15-epochs/model.safetensors:   6%|6         | 15.7MB /  261MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/bhavya777/head-12-64-32-64_colorL-ssim-lpips-mse-smol-unet-15-epochs/commit/3559aafb5cd20e9d3a72710d279593894c765724', commit_message='Push model using huggingface_hub.', commit_description='', oid='3559aafb5cd20e9d3a72710d279593894c765724', pr_url=None, repo_url=RepoUrl('https://huggingface.co/bhavya777/head-12-64-32-64_colorL-ssim-lpips-mse-smol-unet-15-epochs', endpoint='https://huggingface.co', repo_type='model', repo_id='bhavya777/head-12-64-32-64_colorL-ssim-lpips-mse-smol-unet-15-epochs'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "# Replace with your actual token\n",
    "token = \"\"\n",
    "from huggingface_hub import PyTorchModelHubMixin, login\n",
    "\n",
    "# Log in with token\n",
    "login(token=token)\n",
    "\n",
    "# Make your model compatible\n",
    "class HFUNet(PyTorchModelHubMixin, nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = ColorDenoisingUNet()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate and move to device\n",
    "hf_model = HFUNet().to(device)\n",
    "hf_model.model.load_state_dict(model.state_dict())  # copy weights\n",
    "\n",
    "# Push to hub\n",
    "hf_model.push_to_hub(\"head-12-64-32-64_colorL-ssim-lpips-mse-smol-unet-15-epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e9371af-daec-4962-bb0f-cfbfd5e934a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 65,212,403\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary  # if you want detailed stats (optional)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total trainable parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae14bc8-ddd2-49a2-a193-09e184b15a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1 = \n",
    "#model2 - 5epochs,smol-mse-lpips-unet - bhavya777/lpips-mse-smol-unet-5-epochs\n",
    "#model3-ssim-lpips-mse-smol-unet-5-epochs\n",
    "model4-colorL-ssim-lpips-mse-smol-unet-5-epochs\n",
    "model5-colorL-ssim-lpips-mse-big-unet-5-epochss=2\n",
    "model6-layer = 2 in smol unet - layer_big_colorL-ssim-lpips-mse-smol-unet-10-epochs\n",
    "model7-used layers=1,hed_dim=8 usuall smol unet head-8_colorL-ssim-lpips-mse-smol-unet-15-epochs\n",
    "model-8-head-12(64,32,64)_colorL-ssim-lpips-mse-smol-unet-15-epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67c814-becd-45c3-8bd4-3e591cfa6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference \n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import UNet2DConditionModel\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "# Define your model class\n",
    "class ColorDenoisingUNet(nn.Module):\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CLIP text encoder\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "        # UNet model\n",
    "        self.unet = UNet2DConditionModel(\n",
    "            sample_size=128,\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\"),\n",
    "            mid_block_type=\"UNetMidBlock2DCrossAttn\",\n",
    "            up_block_types=(\"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n",
    "            block_out_channels=(64, 128, 256),\n",
    "            layers_per_block=2,\n",
    "            attention_head_dim=8,\n",
    "            cross_attention_dim=512,\n",
    "            only_cross_attention=True,\n",
    "            dropout=0.1,\n",
    "            act_fn=\"silu\",\n",
    "        ).to(device)\n",
    "\n",
    "    def get_text_embedding(self, prompt, x):\n",
    "        if isinstance(prompt, str):\n",
    "            prompt = [prompt] * x.size(0)\n",
    "        text_inputs = self.tokenizer(prompt, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(self.device)\n",
    "        text_embeddings = self.text_encoder(**text_inputs).last_hidden_state\n",
    "        return text_embeddings\n",
    "\n",
    "    def forward(self, x: torch.Tensor, timestep: torch.Tensor, prompt):\n",
    "        encoder_hidden_states = self.get_text_embedding(prompt, x)\n",
    "        return self.unet(sample=x, timestep=timestep, encoder_hidden_states=encoder_hidden_states).sample\n",
    "\n",
    "\n",
    "# Wrapper for Hugging Face Hub loading\n",
    "class HFUNet(PyTorchModelHubMixin, nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = ColorDenoisingUNet()\n",
    "\n",
    "    def forward(self, x, timestep, prompt):\n",
    "        return self.model(x, timestep, prompt)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.model.device\n",
    "\n",
    "\n",
    "# Load model from Hugging Face Hub\n",
    "model = HFUNet.from_pretrained(\"head-12-64-32-64_colorL-ssim-lpips-mse-smol-unet-15-epochs\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Inference function\n",
    "def infer_batch_prompts(model, image_path, prompts):\n",
    "    device = model.device\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_image = transform(image)\n",
    "\n",
    "    input_batch = input_image.unsqueeze(0).repeat(len(prompts), 1, 1, 1).to(device)\n",
    "    timestep = torch.zeros(len(prompts), dtype=torch.long).to(device)\n",
    "    prompt_batch = prompts\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch, timestep, prompt_batch)\n",
    "        output = (output.clamp(-1, 1) + 1) / 2\n",
    "\n",
    "        for i in range(len(prompts)):\n",
    "            out_img = output[i].permute(1, 2, 0).cpu().numpy()\n",
    "            plt.imshow(out_img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Prompt: {prompts[i]}\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdb150-9203-4cdf-a15f-81f4a41427bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
